import json
import pandas as pd
import subprocess
import os
import re
from typing import List, Dict

# -------------------------- é…ç½®å‚æ•°ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰--------------------------
FIO_CONFIG_PATH = "benchmark.fio"  # ä½ çš„FIOé…ç½®æ–‡ä»¶è·¯å¾„
TEST_RUNS = 3  # è¿è¡Œæ¬¡æ•°ï¼ˆå›ºå®š3æ¬¡ï¼‰
JSON_OUTPUT_PREFIX = "fio_results_run"  # æ¯æ¬¡æµ‹è¯•çš„JSONç»“æœå‰ç¼€ï¼ˆå¦‚fio_results_run1.jsonï¼‰
FINAL_EXCEL_PATH = "fioæµ‹è¯•ç»“æœ_3æ¬¡å‡å€¼æ±‡æ€».xlsx"  # æœ€ç»ˆExcelè¾“å‡ºè·¯å¾„
FIO_COMMAND = ["fio", "--output-format=json"]  # FIOåŸºç¡€å‘½ä»¤


# -------------------------- ä¿®å¤ï¼šè§£æFIOé…ç½®æ–‡ä»¶ï¼Œè¿”å›å®Œæ•´å‚æ•° --------------------------
def parse_fio_config() -> tuple[int, int, int, int]:
    """
    è§£æFIOé…ç½®æ–‡ä»¶ï¼Œè·å–ï¼š
    1. runtimeï¼ˆæµ‹è¯•æ—¶é•¿ï¼Œå•ä½ï¼šç§’ï¼‰
    2. ramp_timeï¼ˆé¢„çƒ­æ—¶é•¿ï¼Œå•ä½ï¼šç§’ï¼‰
    3. å•ä¸ªJobçš„æ€»è€—æ—¶ï¼ˆruntime + ramp_timeï¼Œå•ä½ï¼šç§’ï¼‰
    4. è¦è¿è¡Œçš„Jobæ•°é‡ï¼ˆæ’é™¤æ³¨é‡Šã€å…¨å±€é…ç½®ï¼‰
    :return: (runtime, ramp_time, single_job_duration, job_count)
    """
    if not os.path.exists(FIO_CONFIG_PATH):
        raise FileNotFoundError(f"FIOé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{FIO_CONFIG_PATH}")

    # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é… runtime å’Œ ramp_timeï¼ˆæ”¯æŒå¸¦å•ä½s/m/hï¼Œé»˜è®¤sï¼‰
    time_pattern = re.compile(r"(runtime|ramp_time)\s*=\s*(\d+)([smh]?)", re.IGNORECASE)
    # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é…Jobå—ï¼ˆ[job_name] æ ¼å¼ï¼Œæ’é™¤[global]ï¼‰
    job_pattern = re.compile(r"^\s*\[(?!global)\w+", re.MULTILINE)

    runtime = 0
    ramp_time = 0
    job_count = 0

    with open(FIO_CONFIG_PATH, "r", encoding="utf-8") as f:
        content = f.read()

        # 1. æå– runtime å’Œ ramp_time
        matches = time_pattern.findall(content)
        for key, value, unit in matches:
            value = int(value)
            # è½¬æ¢ä¸ºç§’ï¼ˆé»˜è®¤sï¼Œm=60sï¼Œh=3600sï¼‰
            if unit.lower() == "m":
                value *= 60
            elif unit.lower() == "h":
                value *= 3600

            if key.lower() == "runtime":
                runtime = value
            elif key.lower() == "ramp_time":
                ramp_time = value

        # 2. ç»Ÿè®¡Jobæ•°é‡ï¼ˆåŒ¹é…[job_name]æ ¼å¼ï¼Œæ’é™¤[global]ï¼‰
        jobs = job_pattern.findall(content)
        job_count = len(jobs)

    # æ ¡éªŒå‚æ•°ï¼ˆé¿å…é…ç½®æ–‡ä»¶ä¸­æœªè®¾ç½®runtime/ramp_timeï¼‰
    if runtime == 0:
        runtime = 30  # é»˜è®¤30ç§’ï¼ˆè‹¥é…ç½®æ–‡ä»¶æœªè®¾ç½®ï¼‰
        print(f"âš ï¸  æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°runtimeï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼š{runtime}s")
    if ramp_time == 0:
        ramp_time = 5  # é»˜è®¤5ç§’ï¼ˆè‹¥é…ç½®æ–‡ä»¶æœªè®¾ç½®ï¼‰
        print(f"âš ï¸  æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°ramp_timeï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼š{ramp_time}s")
    if job_count == 0:
        raise ValueError("âŒ æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°ä»»ä½•Jobï¼ˆæ ¼å¼åº”ä¸º[job_name]ï¼‰")

    single_job_duration = runtime + ramp_time
    return runtime, ramp_time, single_job_duration, job_count


def calculate_total_estimated_time(single_job_duration: int, job_count: int, test_runs: int) -> str:
    """
    è®¡ç®—æ€»é¢„ä¼°æ—¶é•¿ï¼Œè½¬æ¢ä¸ºã€Œå°æ—¶:åˆ†é’Ÿ:ç§’ã€æ ¼å¼
    :param single_job_duration: å•ä¸ªJobè€—æ—¶ï¼ˆç§’ï¼‰
    :param job_count: Jobæ•°é‡
    :param test_runs: æµ‹è¯•æ¬¡æ•°
    :return: æ ¼å¼åŒ–çš„æ€»é¢„ä¼°æ—¶é•¿å­—ç¬¦ä¸²
    """
    total_seconds = single_job_duration * job_count * test_runs
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60

    if hours > 0:
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’"
    elif minutes > 0:
        return f"{minutes}åˆ†é’Ÿ{seconds}ç§’"
    else:
        return f"{seconds}ç§’"


# -------------------------- åŸæœ‰æ ¸å¿ƒå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰--------------------------
def run_fio_test(run_index: int) -> str:
    """æ‰§è¡Œå•æ¬¡FIOæµ‹è¯•ï¼Œè¿”å›JSONç»“æœæ–‡ä»¶è·¯å¾„"""
    json_path = f"{JSON_OUTPUT_PREFIX}{run_index}.json"
    full_command = FIO_COMMAND + ["--output", json_path, FIO_CONFIG_PATH]

    print(f"\nğŸ“Œ å¼€å§‹ç¬¬{run_index}æ¬¡FIOæµ‹è¯•...")
    print(f"å‘½ä»¤ï¼š{' '.join(full_command)}")

    try:
        result = subprocess.run(
            full_command,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            encoding="utf-8"
        )
        print(f"âœ… ç¬¬{run_index}æ¬¡æµ‹è¯•å®Œæˆï¼Œç»“æœæ–‡ä»¶ï¼š{json_path}")
        return json_path
    except subprocess.CalledProcessError as e:
        print(f"âŒ ç¬¬{run_index}æ¬¡æµ‹è¯•å¤±è´¥ï¼")
        print(f"é”™è¯¯è¾“å‡ºï¼š{e.stderr}")
        raise


def extract_fio_metrics(json_path: str) -> List[Dict]:
    """ä»å•ä¸ªJSONæ–‡ä»¶æå–æŒ‡æ ‡ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰"""
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    jobs = data.get("jobs", [])
    if not jobs:
        print(f"âš ï¸ {json_path} ä¸­æœªæ‰¾åˆ°jobsæ•°æ®ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
        return []

    result_list = []
    for job in jobs:
        job_opts = job.get("job options", {})
        base_info = {
            "groupid": job.get("groupid", ""),
            "æµ‹è¯•åç§°": job.get("jobname", ""),
            "æµ‹è¯•æè¿°": job_opts.get("description", job.get("desc", "")),
            "è¯»å†™æ¨¡å¼": job_opts.get("rw", ""),
            "å—å¤§å°": job_opts.get("bs", ""),
            "IOé˜Ÿåˆ—æ·±åº¦": job_opts.get("iodepth", ""),
            "å¹¶å‘jobæ•°": job_opts.get("numjobs", "")
        }

        read_data = job.get("read", {})
        write_data = job.get("write", {})

        metrics = {
            "è¯»å–é‡(MB)": round(read_data.get("io_kbytes", 0) / 1024, 2),
            "å†™å…¥é‡(MB)": round(write_data.get("io_kbytes", 0) / 1024, 2),
            "è¯»å–å¸¦å®½(MB/s)": round(read_data.get("bw_mean", 0) / 1024, 2),
            "å†™å…¥å¸¦å®½(MB/s)": round(write_data.get("bw_mean", 0) / 1024, 2),
            "è¯»å–IOPS(æ¬¡/ç§’)": round(read_data.get("iops_mean", 0.0), 2),
            "å†™å…¥IOPS(æ¬¡/ç§’)": round(write_data.get("iops_mean", 0.0), 2),
            "æ€»å»¶è¿Ÿå‡å€¼(æ¯«ç§’)": round(read_data.get("lat_ns", {}).get("mean", 0) / 1e6, 2),
            "CPUæ€»ä½¿ç”¨ç‡(%)": round(job.get("usr_cpu", 0) + job.get("sys_cpu", 0), 2)
        }

        result_list.append({**base_info, **metrics})

    print(f"ğŸ“Š ä»{json_path}æå–åˆ° {len(result_list)} ä¸ªJobçš„æŒ‡æ ‡")
    return result_list


def calculate_mean_metrics(all_runs_data: List[List[Dict]]) -> pd.DataFrame:
    """è®¡ç®—3æ¬¡æµ‹è¯•çš„å‡å€¼"""
    combined_data = []
    for run_idx, run_data in enumerate(all_runs_data, 1):
        for job_data in run_data:
            job_data["æµ‹è¯•æ¬¡æ•°"] = run_idx
            combined_data.append(job_data)

    df_combined = pd.DataFrame(combined_data)
    group_keys = ["groupid", "æµ‹è¯•åç§°", "æµ‹è¯•æè¿°", "è¯»å†™æ¨¡å¼", "å—å¤§å°", "IOé˜Ÿåˆ—æ·±åº¦", "å¹¶å‘jobæ•°"]
    metric_cols = [
        "è¯»å–é‡(MB)", "å†™å…¥é‡(MB)", "è¯»å–å¸¦å®½(MB/s)", "å†™å…¥å¸¦å®½(MB/s)",
        "è¯»å–IOPS(æ¬¡/ç§’)", "å†™å…¥IOPS(æ¬¡/ç§’)", "æ€»å»¶è¿Ÿå‡å€¼(æ¯«ç§’)", "CPUæ€»ä½¿ç”¨ç‡(%)"
    ]

    df_mean = df_combined.groupby(group_keys)[metric_cols].mean().round(2).reset_index()
    return df_mean


def generate_final_excel(
        all_runs_data: List[List[Dict]],
        df_mean: pd.DataFrame,
        excel_path: str
):
    """ç”Ÿæˆæœ€ç»ˆExcelï¼ˆ4ä¸ªå·¥ä½œè¡¨ï¼‰"""
    column_order = [
        "groupid", "æµ‹è¯•åç§°", "æµ‹è¯•æè¿°", "è¯»å†™æ¨¡å¼", "å—å¤§å°", "IOé˜Ÿåˆ—æ·±åº¦", "å¹¶å‘jobæ•°",
        "è¯»å–é‡(MB)", "å†™å…¥é‡(MB)",
        "è¯»å–å¸¦å®½(MB/s)", "å†™å…¥å¸¦å®½(MB/s)",
        "è¯»å–IOPS(æ¬¡/ç§’)", "å†™å…¥IOPS(æ¬¡/ç§’)",
        "æ€»å»¶è¿Ÿå‡å€¼(æ¯«ç§’)", "CPUæ€»ä½¿ç”¨ç‡(%)"
    ]

    with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
        # 1. å‡å€¼æ±‡æ€»ï¼ˆç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼‰
        df_mean = df_mean[column_order]
        df_mean.to_excel(writer, sheet_name="å‡å€¼æ±‡æ€»", index=False)

        # 2. 3æ¬¡åŸå§‹æ•°æ®
        for run_idx, run_data in enumerate(all_runs_data, 1):
            sheet_name = f"ç¬¬{run_idx}æ¬¡"
            df_run = pd.DataFrame(run_data)[column_order]
            df_run.to_excel(writer, sheet_name=sheet_name, index=False)

        # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
        for sheet_name in writer.sheets:
            worksheet = writer.sheets[sheet_name]
            for column in worksheet.columns:
                max_length = max(len(str(cell.value)) if cell.value else 0 for cell in column)
                adjusted_width = min(max_length + 3, 25)
                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width

    print(f"\nğŸ‰ æœ€ç»ˆExcelæ–‡ä»¶å·²ç”Ÿæˆï¼š{excel_path}")
    print(f"ğŸ“‹ åŒ…å«å·¥ä½œè¡¨ï¼šå‡å€¼æ±‡æ€»ã€ç¬¬1æ¬¡ã€ç¬¬2æ¬¡ã€ç¬¬3æ¬¡")


# -------------------------- ä¸»æµç¨‹ï¼ˆä¿®å¤å˜é‡ä½œç”¨åŸŸï¼‰--------------------------
def main():
    print("=" * 60)
    print("ğŸš€ å¼€å§‹FIOæµ‹è¯•è‡ªåŠ¨åŒ–æµç¨‹ï¼ˆ3æ¬¡è¿è¡Œ+å‡å€¼æ±‡æ€»ï¼‰")
    print(f"FIOé…ç½®æ–‡ä»¶ï¼š{FIO_CONFIG_PATH}")
    print(f"æœ€ç»ˆExcelè¾“å‡ºï¼š{FINAL_EXCEL_PATH}")
    print("=" * 60)

    try:
        # ä¿®å¤ï¼šè·å– runtime å’Œ ramp_time å˜é‡ï¼ˆä»å‡½æ•°è¿”å›å€¼ä¸­æå–ï¼‰
        print("\nğŸ“Š æ­£åœ¨è§£æFIOé…ç½®æ–‡ä»¶ï¼Œè®¡ç®—é¢„ä¼°æµ‹è¯•æ—¶é•¿...")
        runtime, ramp_time, single_job_duration, job_count = parse_fio_config()
        total_estimated_time = calculate_total_estimated_time(
            single_job_duration, job_count, TEST_RUNS
        )

        # æ‰“å°æ—¶é•¿é¢„ä¼°ä¿¡æ¯ï¼ˆç°åœ¨å˜é‡å¯æ­£å¸¸è®¿é—®ï¼‰
        print(f"âœ… é…ç½®è§£æå®Œæˆï¼š")
        print(f"   - å•ä¸ªJobè€—æ—¶ï¼š{single_job_duration}ç§’ï¼ˆruntime={runtime}s + ramp_time={ramp_time}sï¼‰")
        print(f"   - æ€»Jobæ•°é‡ï¼š{job_count}ä¸ª")
        print(f"   - æµ‹è¯•æ¬¡æ•°ï¼š{TEST_RUNS}æ¬¡")
        print(f"   - æ€»é¢„ä¼°æ—¶é•¿ï¼š{total_estimated_time}ï¼ˆå®é™…æ—¶é•¿å¯èƒ½å› ç³»ç»Ÿè´Ÿè½½ç•¥æœ‰å·®å¼‚ï¼‰")

        # ç¡®è®¤æ˜¯å¦ç»§ç»­
        confirm = input("\nâ“ æ˜¯å¦ç»§ç»­æ‰§è¡Œæµ‹è¯•ï¼Ÿ(y/nï¼Œé»˜è®¤y) ").lower()
        if confirm != "y" and confirm != "":
            print("ğŸ›‘ æµ‹è¯•å·²å–æ¶ˆ")
            return

        # åŸæœ‰æ­¥éª¤ï¼šæ‰§è¡Œæµ‹è¯•ã€æå–æ•°æ®ã€è®¡ç®—å‡å€¼ã€ç”ŸæˆExcel
        json_paths = []
        for run_idx in range(1, TEST_RUNS + 1):
            json_path = run_fio_test(run_idx)
            json_paths.append(json_path)

        all_runs_data = []
        for json_path in json_paths:
            run_data = extract_fio_metrics(json_path)
            if run_data:
                all_runs_data.append(run_data)

        print("\nğŸ“ˆ å¼€å§‹è®¡ç®—3æ¬¡æµ‹è¯•å‡å€¼...")
        df_mean = calculate_mean_metrics(all_runs_data)

        generate_final_excel(all_runs_data, df_mean, FINAL_EXCEL_PATH)

        # å¯é€‰ï¼šåˆ é™¤ä¸­é—´JSONæ–‡ä»¶
        if input("\nâ“ æ˜¯å¦åˆ é™¤ä¸­é—´JSONç»“æœæ–‡ä»¶ï¼Ÿ(y/nï¼Œé»˜è®¤n) ").lower() == "y":
            for json_path in json_paths:
                os.remove(json_path)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶ï¼š{json_path}")

        print("\nâœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ æµç¨‹æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        raise


if __name__ == "__main__":
    main()